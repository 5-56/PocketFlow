#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¼‚æ­¥LLMè°ƒç”¨æ±  - é«˜æ€§èƒ½LLMè°ƒç”¨ç®¡ç†
æ”¯æŒè¿æ¥æ± ã€æ™ºèƒ½ç¼“å­˜ã€é€Ÿç‡é™åˆ¶ã€æ•…éšœè½¬ç§»
"""

import asyncio
import aiohttp
import hashlib
import json
import time
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from contextlib import asynccontextmanager
import psutil
import os

logger = logging.getLogger(__name__)

@dataclass
class LLMResponse:
    """LLMå“åº”ç»“æœ"""
    content: str
    model: str
    tokens_used: int
    response_time: float
    from_cache: bool = False
    cost_estimate: float = 0.0

@dataclass
class LLMStats:
    """LLMè°ƒç”¨ç»Ÿè®¡"""
    total_requests: int = 0
    cache_hits: int = 0
    failures: int = 0
    total_tokens: int = 0
    total_cost: float = 0.0
    avg_response_time: float = 0.0

class AsyncLLMPool:
    """é«˜æ€§èƒ½å¼‚æ­¥LLMè°ƒç”¨æ± """
    
    def __init__(self, 
                 max_connections: int = 20,
                 cache_size: int = 1000,
                 cache_ttl: int = 3600,
                 rate_limit: int = 60,
                 max_retries: int = 3):
        
        self.max_connections = max_connections
        self.cache_size = cache_size
        self.cache_ttl = cache_ttl
        self.rate_limit = rate_limit
        self.max_retries = max_retries
        
        # ç¼“å­˜ç³»ç»Ÿ
        self.cache: Dict[str, tuple] = {}
        
        # é€Ÿç‡é™åˆ¶
        self.rate_semaphore = asyncio.Semaphore(rate_limit)
        self.request_times: List[float] = []
        
        # è¿æ¥æ± 
        self.connector = None
        self.session = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = LLMStats()
        
        # æ”¯æŒçš„æ¨¡å‹é…ç½®
        self.model_config = {
            "gpt-4o": {"cost_per_1k": 0.005, "max_tokens": 4096},
            "gpt-4o-mini": {"cost_per_1k": 0.0015, "max_tokens": 4096},
            "gpt-3.5-turbo": {"cost_per_1k": 0.001, "max_tokens": 4096}
        }
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.connector = aiohttp.TCPConnector(
            limit=self.max_connections,
            limit_per_host=self.max_connections,
            enable_cleanup_closed=True
        )
        timeout = aiohttp.ClientTimeout(total=60, connect=10)
        self.session = aiohttp.ClientSession(
            connector=self.connector,
            timeout=timeout
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
        if self.connector:
            await self.connector.close()
    
    def _get_cache_key(self, prompt: str, model: str, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # æ’é™¤éç¡®å®šæ€§å‚æ•°
        cache_params = {k: v for k, v in kwargs.items() 
                       if k not in ['stream', 'user']}
        
        cache_data = {
            "prompt": prompt,
            "model": model,
            **cache_params
        }
        
        cache_str = json.dumps(cache_data, sort_keys=True)
        return hashlib.md5(cache_str.encode()).hexdigest()
    
    def _get_cached_response(self, cache_key: str) -> Optional[LLMResponse]:
        """è·å–ç¼“å­˜çš„å“åº”"""
        if cache_key in self.cache:
            response, timestamp = self.cache[cache_key]
            if time.time() - timestamp < self.cache_ttl:
                # åˆ›å»ºæ–°çš„å“åº”å¯¹è±¡å¹¶æ ‡è®°ä¸ºç¼“å­˜
                cached_response = LLMResponse(
                    content=response.content,
                    model=response.model,
                    tokens_used=response.tokens_used,
                    response_time=response.response_time,
                    from_cache=True,
                    cost_estimate=response.cost_estimate
                )
                self.stats.cache_hits += 1
                return cached_response
            else:
                # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
                del self.cache[cache_key]
        return None
    
    def _cache_response(self, cache_key: str, response: LLMResponse):
        """ç¼“å­˜å“åº” - LRUç­–ç•¥"""
        if len(self.cache) >= self.cache_size:
            # åˆ é™¤æœ€æ—§çš„æ¡ç›®
            oldest_key = min(self.cache.keys(), 
                           key=lambda k: self.cache[k][1])
            del self.cache[oldest_key]
        
        self.cache[cache_key] = (response, time.time())
    
    async def _wait_for_rate_limit(self):
        """æ™ºèƒ½é€Ÿç‡é™åˆ¶"""
        now = time.time()
        
        # æ¸…ç†è¶…è¿‡1åˆ†é’Ÿçš„è¯·æ±‚è®°å½•
        self.request_times = [t for t in self.request_times 
                            if now - t < 60]
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é€Ÿç‡é™åˆ¶
        if len(self.request_times) >= self.rate_limit:
            sleep_time = 60 - (now - self.request_times[0])
            if sleep_time > 0:
                logger.info(f"Rate limit reached, sleeping for {sleep_time:.2f}s")
                await asyncio.sleep(sleep_time)
        
        self.request_times.append(now)
    
    def _calculate_cost(self, model: str, tokens: int) -> float:
        """è®¡ç®—APIè°ƒç”¨æˆæœ¬"""
        if model in self.model_config:
            cost_per_1k = self.model_config[model]["cost_per_1k"]
            return (tokens / 1000) * cost_per_1k
        return 0.0
    
    async def call_llm_async(self, 
                           prompt: str, 
                           model: str = "gpt-4o-mini",
                           max_tokens: int = 3000,
                           temperature: float = 0.7,
                           **kwargs) -> LLMResponse:
        """å¼‚æ­¥è°ƒç”¨LLM"""
        start_time = time.time()
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._get_cache_key(
            prompt, model, max_tokens=max_tokens, 
            temperature=temperature, **kwargs
        )
        
        # æ£€æŸ¥ç¼“å­˜
        cached_response = self._get_cached_response(cache_key)
        if cached_response:
            logger.debug(f"Cache hit for prompt: {prompt[:50]}...")
            return cached_response
        
        # é€Ÿç‡é™åˆ¶
        await self._wait_for_rate_limit()
        
        # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘
        async with self.rate_semaphore:
            # é‡è¯•æœºåˆ¶
            last_error = None
            for attempt in range(self.max_retries):
                try:
                    response_data = await self._make_llm_request(
                        prompt, model, max_tokens, temperature, **kwargs
                    )
                    
                    # è®¡ç®—å“åº”æ—¶é—´å’Œæˆæœ¬
                    response_time = time.time() - start_time
                    cost = self._calculate_cost(model, response_data.get("tokens_used", 0))
                    
                    # åˆ›å»ºå“åº”å¯¹è±¡
                    llm_response = LLMResponse(
                        content=response_data["content"],
                        model=model,
                        tokens_used=response_data.get("tokens_used", 0),
                        response_time=response_time,
                        cost_estimate=cost
                    )
                    
                    # ç¼“å­˜ç»“æœ
                    self._cache_response(cache_key, llm_response)
                    
                    # æ›´æ–°ç»Ÿè®¡
                    self._update_stats(llm_response, success=True)
                    
                    logger.debug(f"LLM call successful in {response_time:.2f}s, "
                               f"tokens: {llm_response.tokens_used}, "
                               f"cost: ${cost:.4f}")
                    
                    return llm_response
                    
                except Exception as e:
                    last_error = e
                    logger.warning(f"LLMè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {e}")
                    
                    if attempt < self.max_retries - 1:
                        # æŒ‡æ•°é€€é¿
                        sleep_time = (2 ** attempt) + (time.time() % 1)
                        await asyncio.sleep(sleep_time)
            
            # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
            self.stats.failures += 1
            logger.error(f"LLMè°ƒç”¨æœ€ç»ˆå¤±è´¥: {last_error}")
            raise last_error
    
    async def _make_llm_request(self, prompt: str, model: str, max_tokens: int, 
                              temperature: float, **kwargs) -> Dict[str, Any]:
        """å®é™…çš„LLM APIè°ƒç”¨"""
        try:
            from openai import AsyncOpenAI
        except ImportError:
            raise ImportError("éœ€è¦å®‰è£… openai åº“: pip install openai")
        
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("æœªè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        
        client = AsyncOpenAI(api_key=api_key)
        
        # æ„å»ºæ¶ˆæ¯
        messages = [{"role": "user", "content": prompt}]
        if "system" in kwargs:
            messages.insert(0, {"role": "system", "content": kwargs.pop("system")})
        
        # APIè°ƒç”¨
        response = await client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
            **kwargs
        )
        
        return {
            "content": response.choices[0].message.content,
            "tokens_used": response.usage.total_tokens if response.usage else 0
        }
    
    async def batch_call_llm_async(self, 
                                 prompts: List[str],
                                 model: str = "gpt-4o-mini",
                                 max_tokens: int = 3000,
                                 temperature: float = 0.7,
                                 max_concurrent: int = 5,
                                 **kwargs) -> List[LLMResponse]:
        """æ‰¹é‡å¼‚æ­¥è°ƒç”¨LLM"""
        if not prompts:
            return []
        
        # åˆ›å»ºå¹¶å‘é™åˆ¶ä¿¡å·é‡
        concurrent_semaphore = asyncio.Semaphore(max_concurrent)
        
        async def limited_call(prompt):
            async with concurrent_semaphore:
                return await self.call_llm_async(
                    prompt, model, max_tokens, temperature, **kwargs
                )
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç† {len(prompts)} ä¸ªè¯·æ±‚ï¼Œå¹¶å‘æ•°: {max_concurrent}")
        start_time = time.time()
        
        try:
            responses = await asyncio.gather(
                *[limited_call(prompt) for prompt in prompts],
                return_exceptions=True
            )
            
            # å¤„ç†å¼‚å¸¸
            successful_responses = []
            failed_count = 0
            
            for i, response in enumerate(responses):
                if isinstance(response, Exception):
                    logger.error(f"æ‰¹é‡è¯·æ±‚ {i} å¤±è´¥: {response}")
                    failed_count += 1
                    # å¯ä»¥é€‰æ‹©æ·»åŠ é»˜è®¤å“åº”æˆ–è€…è·³è¿‡
                    successful_responses.append(LLMResponse(
                        content=f"å¤„ç†å¤±è´¥: {str(response)}",
                        model=model,
                        tokens_used=0,
                        response_time=0,
                        from_cache=False
                    ))
                else:
                    successful_responses.append(response)
            
            total_time = time.time() - start_time
            logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆ: {len(successful_responses) - failed_count}/"
                       f"{len(prompts)} æˆåŠŸï¼Œè€—æ—¶ {total_time:.2f}s")
            
            return successful_responses
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†å‘ç”Ÿå¼‚å¸¸: {e}")
            raise
    
    def _update_stats(self, response: LLMResponse, success: bool = True):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.stats.total_requests += 1
        
        if success:
            self.stats.total_tokens += response.tokens_used
            self.stats.total_cost += response.cost_estimate
            
            # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
            if not response.from_cache:
                current_avg = self.stats.avg_response_time
                total_non_cached = self.stats.total_requests - self.stats.cache_hits
                
                if total_non_cached > 0:
                    self.stats.avg_response_time = (
                        (current_avg * (total_non_cached - 1) + response.response_time) /
                        total_non_cached
                    )
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        cache_hit_rate = (self.stats.cache_hits / max(1, self.stats.total_requests)) * 100
        failure_rate = (self.stats.failures / max(1, self.stats.total_requests)) * 100
        
        return {
            "total_requests": self.stats.total_requests,
            "cache_hits": self.stats.cache_hits,
            "cache_hit_rate": f"{cache_hit_rate:.2f}%",
            "failures": self.stats.failures,
            "failure_rate": f"{failure_rate:.2f}%",
            "total_tokens": self.stats.total_tokens,
            "total_cost": f"${self.stats.total_cost:.4f}",
            "avg_response_time": f"{self.stats.avg_response_time:.2f}s",
            "cached_items": len(self.cache),
            "memory_usage": f"{psutil.Process().memory_info().rss / 1024 / 1024:.2f}MB"
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        logger.info("LLMç¼“å­˜å·²æ¸…ç©º")
    
    def get_cache_size(self) -> int:
        """è·å–å½“å‰ç¼“å­˜å¤§å°"""
        return len(self.cache)

# å…¨å±€å•ä¾‹å®ä¾‹
_global_llm_pool = None

async def get_global_llm_pool() -> AsyncLLMPool:
    """è·å–å…¨å±€LLMæ± å®ä¾‹"""
    global _global_llm_pool
    if _global_llm_pool is None:
        _global_llm_pool = AsyncLLMPool()
        await _global_llm_pool.__aenter__()
    return _global_llm_pool

async def call_llm_async(prompt: str, **kwargs) -> str:
    """ä¾¿æ·çš„å¼‚æ­¥LLMè°ƒç”¨å‡½æ•°"""
    pool = await get_global_llm_pool()
    response = await pool.call_llm_async(prompt, **kwargs)
    return response.content

async def batch_call_llm_async(prompts: List[str], **kwargs) -> List[str]:
    """ä¾¿æ·çš„æ‰¹é‡å¼‚æ­¥LLMè°ƒç”¨å‡½æ•°"""
    pool = await get_global_llm_pool()
    responses = await pool.batch_call_llm_async(prompts, **kwargs)
    return [response.content for response in responses]

async def get_llm_stats() -> Dict[str, Any]:
    """è·å–LLMè°ƒç”¨ç»Ÿè®¡ä¿¡æ¯"""
    pool = await get_global_llm_pool()
    return pool.get_stats()

async def clear_llm_cache():
    """æ¸…ç©ºLLMç¼“å­˜"""
    pool = await get_global_llm_pool()
    pool.clear_cache()

if __name__ == "__main__":
    async def test_async_llm_pool():
        """æµ‹è¯•å¼‚æ­¥LLMæ± """
        async with AsyncLLMPool(rate_limit=10) as pool:
            print("ğŸ§ª æµ‹è¯•å¼‚æ­¥LLMè°ƒç”¨æ± ")
            
            # å•ä¸ªè°ƒç”¨æµ‹è¯•
            print("\nğŸ“ æµ‹è¯•å•ä¸ªè°ƒç”¨...")
            response = await pool.call_llm_async(
                "è¯·ç”¨ä¸€å¥è¯ä»‹ç»Pythonç¼–ç¨‹è¯­è¨€", 
                model="gpt-4o-mini"
            )
            print(f"å“åº”: {response.content}")
            print(f"è€—æ—¶: {response.response_time:.2f}s")
            print(f"Tokens: {response.tokens_used}")
            print(f"æˆæœ¬: ${response.cost_estimate:.4f}")
            
            # ç¼“å­˜æµ‹è¯•
            print("\nğŸ’¾ æµ‹è¯•ç¼“å­˜åŠŸèƒ½...")
            cached_response = await pool.call_llm_async(
                "è¯·ç”¨ä¸€å¥è¯ä»‹ç»Pythonç¼–ç¨‹è¯­è¨€", 
                model="gpt-4o-mini"
            )
            print(f"ç¼“å­˜å‘½ä¸­: {cached_response.from_cache}")
            print(f"å“åº”æ—¶é—´: {cached_response.response_time:.2f}s")
            
            # æ‰¹é‡è°ƒç”¨æµ‹è¯•
            print("\nğŸ“¦ æµ‹è¯•æ‰¹é‡è°ƒç”¨...")
            prompts = [
                "ä»‹ç»æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ",
                "è§£é‡Šæ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ çš„åŒºåˆ«", 
                "æè¿°äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨"
            ]
            
            batch_responses = await pool.batch_call_llm_async(
                prompts, 
                model="gpt-4o-mini",
                max_concurrent=2
            )
            
            for i, response in enumerate(batch_responses):
                print(f"æ‰¹é‡å“åº” {i+1}: {response.content[:50]}...")
                print(f"  è€—æ—¶: {response.response_time:.2f}s, Tokens: {response.tokens_used}")
            
            # ç»Ÿè®¡ä¿¡æ¯
            print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            stats = pool.get_stats()
            for key, value in stats.items():
                print(f"  {key}: {value}")
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_async_llm_pool())