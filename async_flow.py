#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¼‚æ­¥æ–‡æ¡£å¤„ç†å·¥ä½œæµ - é«˜æ€§èƒ½ç‰ˆæœ¬
åŸºäºPocketFlow AsyncFlowå®ç°çš„æ–°ä¸€ä»£æ–‡æ¡£å¤„ç†å·¥ä½œæµ
"""

import asyncio
import logging
from typing import Dict, Any, Optional, List
from pocketflow import AsyncFlow, AsyncParallelBatchFlow

# å¯¼å…¥å¼‚æ­¥èŠ‚ç‚¹
from async_nodes import (
    AsyncParseRequirementNode,
    AsyncAnalyzeDocumentNode,
    AsyncDesignLayoutNode,
    AsyncProcessTextNode,
    ParallelImageProcessingNode,
    AsyncGenerateDocumentNode
)

logger = logging.getLogger(__name__)

class DocumentProcessingAsyncFlow(AsyncFlow):
    """æ–‡æ¡£å¤„ç†å¼‚æ­¥å·¥ä½œæµ"""
    
    def __init__(self, processing_strategy: str = "complete"):
        self.processing_strategy = processing_strategy
        
        # åˆ›å»ºå¼‚æ­¥èŠ‚ç‚¹å®ä¾‹
        self.parse_requirement = AsyncParseRequirementNode(
            max_retries=2, wait=1
        )
        self.analyze_document = AsyncAnalyzeDocumentNode(
            max_retries=2, wait=1
        )
        self.design_layout = AsyncDesignLayoutNode(
            max_retries=3, wait=2
        )
        self.process_text = AsyncProcessTextNode(
            max_retries=2, wait=1
        )
        self.unify_images = ParallelImageProcessingNode(
            max_retries=2, wait=1
        )
        self.generate_document = AsyncGenerateDocumentNode(
            max_retries=2, wait=1
        )
        
        # æ ¹æ®ç­–ç•¥æ„å»ºä¸åŒçš„å·¥ä½œæµ
        self._build_workflow()
        
        # åˆå§‹åŒ–çˆ¶ç±»
        super().__init__(start=self.parse_requirement)
    
    def _build_workflow(self):
        """æ ¹æ®å¤„ç†ç­–ç•¥æ„å»ºå·¥ä½œæµ"""
        if self.processing_strategy == "complete":
            # å®Œæ•´æµç¨‹ï¼šéœ€æ±‚è§£æ -> æ–‡æ¡£åˆ†æ -> è®¾è®¡å¸ƒå±€ -> æ–‡æœ¬å¤„ç† -> å›¾ç‰‡å¤„ç† -> æ–‡æ¡£ç”Ÿæˆ
            self.parse_requirement >> self.analyze_document >> self.design_layout >> self.process_text >> self.unify_images >> self.generate_document
            
        elif self.processing_strategy == "quick":
            # å¿«é€Ÿæµç¨‹ï¼šéœ€æ±‚è§£æ -> è®¾è®¡å¸ƒå±€ -> æ–‡æœ¬å¤„ç† -> æ–‡æ¡£ç”Ÿæˆ
            self.parse_requirement >> self.design_layout >> self.process_text >> self.generate_document
            
        elif self.processing_strategy == "text_only":
            # ä»…æ–‡æœ¬å¤„ç†ï¼šéœ€æ±‚è§£æ -> æ–‡æœ¬å¤„ç† -> æ–‡æ¡£ç”Ÿæˆ
            self.parse_requirement >> self.process_text >> self.generate_document
            
        elif self.processing_strategy == "analysis_focus":
            # åˆ†æé‡ç‚¹ï¼šéœ€æ±‚è§£æ -> æ–‡æ¡£åˆ†æ -> è®¾è®¡å¸ƒå±€ -> æ–‡æ¡£ç”Ÿæˆ
            self.parse_requirement >> self.analyze_document >> self.design_layout >> self.generate_document
            
        else:
            # é»˜è®¤å®Œæ•´æµç¨‹
            self.parse_requirement >> self.analyze_document >> self.design_layout >> self.process_text >> self.unify_images >> self.generate_document
    
    async def run_async(self, shared: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œå¼‚æ­¥å·¥ä½œæµ"""
        logger.info(f"å¯åŠ¨å¼‚æ­¥æ–‡æ¡£å¤„ç†æµç¨‹: {self.processing_strategy}")
        
        # æ·»åŠ æµç¨‹å…ƒæ•°æ®
        shared["workflow_metadata"] = {
            "strategy": self.processing_strategy,
            "start_time": asyncio.get_event_loop().time(),
            "async_mode": True
        }
        
        try:
            # è¿è¡Œå¼‚æ­¥æµç¨‹
            await super().run_async(shared)
            
            # è®°å½•å®Œæˆæ—¶é—´
            end_time = asyncio.get_event_loop().time()
            shared["workflow_metadata"]["end_time"] = end_time
            shared["workflow_metadata"]["total_time"] = end_time - shared["workflow_metadata"]["start_time"]
            
            logger.info(f"å¼‚æ­¥æµç¨‹å®Œæˆï¼Œè€—æ—¶: {shared['workflow_metadata']['total_time']:.2f}ç§’")
            
            return shared
            
        except Exception as e:
            logger.error(f"å¼‚æ­¥æµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            shared["workflow_metadata"]["error"] = str(e)
            raise

class BatchDocumentProcessingFlow(AsyncParallelBatchFlow):
    """æ‰¹é‡æ–‡æ¡£å¤„ç†å¼‚æ­¥å·¥ä½œæµ"""
    
    def __init__(self, processing_strategy: str = "complete", max_concurrent: int = 3):
        self.processing_strategy = processing_strategy
        self.max_concurrent = max_concurrent
        
        # åˆ›å»ºå•æ–‡æ¡£å¤„ç†æµç¨‹
        self.single_doc_flow = DocumentProcessingAsyncFlow(processing_strategy)
        
        # åˆå§‹åŒ–çˆ¶ç±»
        super().__init__(start=self.single_doc_flow)
    
    async def prep_async(self, shared):
        """å‡†å¤‡æ‰¹é‡å¤„ç†å‚æ•°"""
        documents = shared.get("documents", [])
        
        if not documents:
            logger.warning("æ²¡æœ‰æä¾›æ–‡æ¡£è¿›è¡Œæ‰¹é‡å¤„ç†")
            return []
        
        # ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ›å»ºå‚æ•°å­—å…¸
        batch_params = []
        for i, doc in enumerate(documents):
            doc_params = {
                "document_id": f"doc_{i}",
                "original_document": doc.get("content", ""),
                "file_type": doc.get("file_type", "markdown"),
                "user_instruction": shared.get("user_instruction", ""),
                "batch_index": i,
                "total_documents": len(documents)
            }
            batch_params.append(doc_params)
        
        logger.info(f"å‡†å¤‡æ‰¹é‡å¤„ç† {len(batch_params)} ä¸ªæ–‡æ¡£")
        return batch_params
    
    async def post_async(self, shared, prep_res, exec_res_list):
        """å¤„ç†æ‰¹é‡ç»“æœ"""
        successful_docs = []
        failed_docs = []
        
        for i, result in enumerate(exec_res_list):
            if isinstance(result, Exception):
                failed_docs.append({
                    "index": i,
                    "error": str(result)
                })
            else:
                successful_docs.append({
                    "index": i,
                    "result": result.get("final_document", {}),
                    "metadata": result.get("workflow_metadata", {})
                })
        
        # ä¿å­˜æ‰¹é‡å¤„ç†ç»“æœ
        shared["batch_results"] = {
            "successful_count": len(successful_docs),
            "failed_count": len(failed_docs),
            "successful_documents": successful_docs,
            "failed_documents": failed_docs,
            "total_processing_time": sum(
                doc["metadata"].get("total_time", 0) for doc in successful_docs
            ),
            "average_processing_time": sum(
                doc["metadata"].get("total_time", 0) for doc in successful_docs
            ) / max(1, len(successful_docs))
        }
        
        logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆ: {len(successful_docs)}/{len(exec_res_list)} æˆåŠŸ")
        return "default"

class IntelligentWorkflowSelector:
    """æ™ºèƒ½å·¥ä½œæµé€‰æ‹©å™¨"""
    
    @staticmethod
    async def analyze_requirements(user_instruction: str, document_content: str) -> str:
        """åˆ†æéœ€æ±‚å¹¶æ¨èæœ€é€‚åˆçš„å·¥ä½œæµç­–ç•¥"""
        from utils.async_llm_pool import call_llm_async
        
        analysis_prompt = f"""
åˆ†æä»¥ä¸‹ç”¨æˆ·éœ€æ±‚å’Œæ–‡æ¡£å†…å®¹ï¼Œæ¨èæœ€é€‚åˆçš„å¤„ç†ç­–ç•¥ï¼š

ç”¨æˆ·æŒ‡ä»¤: "{user_instruction}"
æ–‡æ¡£é•¿åº¦: {len(document_content)}å­—ç¬¦
æ–‡æ¡£é¢„è§ˆ: {document_content[:200]}...

å¯é€‰ç­–ç•¥:
1. complete - å®Œæ•´å¤„ç†ï¼ˆé€‚åˆå¤æ‚æ–‡æ¡£å’Œé«˜è´¨é‡è¦æ±‚ï¼‰
2. quick - å¿«é€Ÿå¤„ç†ï¼ˆé€‚åˆç®€å•éœ€æ±‚å’Œæ—¶é—´æ•æ„Ÿï¼‰
3. text_only - ä»…æ–‡æœ¬å¤„ç†ï¼ˆé€‚åˆçº¯æ–‡æœ¬ä¼˜åŒ–ï¼‰
4. analysis_focus - åˆ†æé‡ç‚¹ï¼ˆé€‚åˆéœ€è¦æ·±åº¦åˆ†æçš„æ–‡æ¡£ï¼‰

è¯·æ ¹æ®ä»¥ä¸‹å› ç´ æ¨èç­–ç•¥ï¼š
- ç”¨æˆ·æŒ‡ä»¤çš„å¤æ‚åº¦
- æ–‡æ¡£çš„é•¿åº¦å’Œå¤æ‚æ€§
- æ˜¯å¦æ¶‰åŠå›¾ç‰‡å¤„ç†
- å¤„ç†è´¨é‡è¦æ±‚

è¿”å›JSONæ ¼å¼ï¼š
{{
    "recommended_strategy": "ç­–ç•¥åç§°",
    "reasoning": "æ¨èç†ç”±",
    "estimated_time": "é¢„ä¼°å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰",
    "confidence": "ç½®ä¿¡åº¦ 0-100"
}}
"""
        
        try:
            result = await call_llm_async(
                analysis_prompt,
                model="gpt-4o-mini",
                temperature=0.2
            )
            
            if "```json" in result:
                json_str = result.split("```json")[1].split("```")[0].strip()
            else:
                json_str = result
            
            import json
            analysis = json.loads(json_str)
            
            recommended_strategy = analysis.get("recommended_strategy", "complete")
            logger.info(f"æ™ºèƒ½æ¨èç­–ç•¥: {recommended_strategy}, ç†ç”±: {analysis.get('reasoning', '')}")
            
            return recommended_strategy
            
        except Exception as e:
            logger.warning(f"ç­–ç•¥åˆ†æå¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤ç­–ç•¥")
            return "complete"

def create_async_document_flow(strategy: str = "complete") -> DocumentProcessingAsyncFlow:
    """åˆ›å»ºå¼‚æ­¥æ–‡æ¡£å¤„ç†å·¥ä½œæµ"""
    return DocumentProcessingAsyncFlow(strategy)

def create_batch_async_flow(strategy: str = "complete", max_concurrent: int = 3) -> BatchDocumentProcessingFlow:
    """åˆ›å»ºæ‰¹é‡å¼‚æ­¥å¤„ç†å·¥ä½œæµ"""
    return BatchDocumentProcessingFlow(strategy, max_concurrent)

async def auto_create_optimal_flow(user_instruction: str, document_content: str) -> DocumentProcessingAsyncFlow:
    """è‡ªåŠ¨åˆ›å»ºæœ€ä¼˜å·¥ä½œæµ"""
    selector = IntelligentWorkflowSelector()
    optimal_strategy = await selector.analyze_requirements(user_instruction, document_content)
    return create_async_document_flow(optimal_strategy)

def get_async_flow_by_type(flow_type: str = "complete", **kwargs) -> DocumentProcessingAsyncFlow:
    """æ ¹æ®ç±»å‹è·å–å¼‚æ­¥å·¥ä½œæµ"""
    flows = {
        "complete": lambda: create_async_document_flow("complete"),
        "quick": lambda: create_async_document_flow("quick"),
        "text_only": lambda: create_async_document_flow("text_only"),
        "analysis_focus": lambda: create_async_document_flow("analysis_focus"),
        "batch": lambda: create_batch_async_flow(
            kwargs.get("strategy", "complete"), 
            kwargs.get("max_concurrent", 3)
        )
    }
    
    return flows.get(flow_type, flows["complete"])()

# é«˜çº§å·¥ä½œæµåŠŸèƒ½
class AdaptiveWorkflow:
    """è‡ªé€‚åº”å·¥ä½œæµ"""
    
    def __init__(self):
        self.performance_history = []
        self.current_load = 0
    
    async def create_adaptive_flow(self, shared: Dict[str, Any]) -> DocumentProcessingAsyncFlow:
        """æ ¹æ®ç³»ç»Ÿè´Ÿè½½å’Œå†å²æ€§èƒ½åˆ›å»ºè‡ªé€‚åº”å·¥ä½œæµ"""
        # åˆ†æå½“å‰ç³»ç»ŸçŠ¶æ€
        system_load = await self._analyze_system_load()
        content_complexity = self._analyze_content_complexity(shared)
        
        # é€‰æ‹©æœ€ä¼˜ç­–ç•¥
        if system_load > 0.8:  # é«˜è´Ÿè½½
            strategy = "quick"
        elif content_complexity > 0.7:  # é«˜å¤æ‚åº¦
            strategy = "complete"
        else:
            strategy = "analysis_focus"
        
        logger.info(f"è‡ªé€‚åº”å·¥ä½œæµé€‰æ‹©ç­–ç•¥: {strategy} (è´Ÿè½½: {system_load}, å¤æ‚åº¦: {content_complexity})")
        
        return create_async_document_flow(strategy)
    
    async def _analyze_system_load(self) -> float:
        """åˆ†æç³»ç»Ÿè´Ÿè½½"""
        try:
            import psutil
            cpu_percent = psutil.cpu_percent(interval=1)
            memory_percent = psutil.virtual_memory().percent
            
            # ç»¼åˆCPUå’Œå†…å­˜ä½¿ç”¨ç‡
            system_load = (cpu_percent + memory_percent) / 200.0
            return min(system_load, 1.0)
            
        except ImportError:
            return 0.5  # é»˜è®¤ä¸­ç­‰è´Ÿè½½
    
    def _analyze_content_complexity(self, shared: Dict[str, Any]) -> float:
        """åˆ†æå†…å®¹å¤æ‚åº¦"""
        content = shared.get("original_document", "")
        instruction = shared.get("user_instruction", "")
        
        # ç®€å•å¤æ‚åº¦è¯„åˆ†
        factors = [
            len(content) / 10000,  # å†…å®¹é•¿åº¦
            len(instruction.split()) / 50,  # æŒ‡ä»¤å¤æ‚åº¦
            content.count('\n') / 100,  # ç»“æ„å¤æ‚åº¦
            content.count('![') / 10,  # å›¾ç‰‡æ•°é‡
        ]
        
        complexity = sum(factors) / len(factors)
        return min(complexity, 1.0)

# æµç¨‹ç›‘æ§å’Œä¼˜åŒ–
class WorkflowMonitor:
    """å·¥ä½œæµç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics = {
            "total_runs": 0,
            "successful_runs": 0,
            "failed_runs": 0,
            "average_time": 0.0,
            "strategy_performance": {}
        }
    
    async def monitor_flow_execution(self, flow: DocumentProcessingAsyncFlow, shared: Dict[str, Any]):
        """ç›‘æ§å·¥ä½œæµæ‰§è¡Œ"""
        start_time = asyncio.get_event_loop().time()
        strategy = flow.processing_strategy
        
        try:
            await flow.run_async(shared)
            
            # è®°å½•æˆåŠŸæŒ‡æ ‡
            execution_time = asyncio.get_event_loop().time() - start_time
            self._record_success(strategy, execution_time)
            
            logger.info(f"å·¥ä½œæµç›‘æ§: {strategy} ç­–ç•¥æ‰§è¡ŒæˆåŠŸï¼Œè€—æ—¶ {execution_time:.2f}s")
            
        except Exception as e:
            # è®°å½•å¤±è´¥æŒ‡æ ‡
            self._record_failure(strategy)
            logger.error(f"å·¥ä½œæµç›‘æ§: {strategy} ç­–ç•¥æ‰§è¡Œå¤±è´¥ - {e}")
            raise
    
    def _record_success(self, strategy: str, execution_time: float):
        """è®°å½•æˆåŠŸæ‰§è¡Œ"""
        self.metrics["total_runs"] += 1
        self.metrics["successful_runs"] += 1
        
        # æ›´æ–°å¹³å‡æ—¶é—´
        current_avg = self.metrics["average_time"]
        total_successful = self.metrics["successful_runs"]
        self.metrics["average_time"] = (
            (current_avg * (total_successful - 1) + execution_time) / total_successful
        )
        
        # æ›´æ–°ç­–ç•¥æ€§èƒ½
        if strategy not in self.metrics["strategy_performance"]:
            self.metrics["strategy_performance"][strategy] = {
                "runs": 0, "successes": 0, "avg_time": 0.0
            }
        
        strategy_metrics = self.metrics["strategy_performance"][strategy]
        strategy_metrics["runs"] += 1
        strategy_metrics["successes"] += 1
        
        # æ›´æ–°ç­–ç•¥å¹³å‡æ—¶é—´
        strategy_avg = strategy_metrics["avg_time"]
        strategy_successes = strategy_metrics["successes"]
        strategy_metrics["avg_time"] = (
            (strategy_avg * (strategy_successes - 1) + execution_time) / strategy_successes
        )
    
    def _record_failure(self, strategy: str):
        """è®°å½•æ‰§è¡Œå¤±è´¥"""
        self.metrics["total_runs"] += 1
        self.metrics["failed_runs"] += 1
        
        if strategy not in self.metrics["strategy_performance"]:
            self.metrics["strategy_performance"][strategy] = {
                "runs": 0, "successes": 0, "avg_time": 0.0
            }
        
        self.metrics["strategy_performance"][strategy]["runs"] += 1
    
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        success_rate = (self.metrics["successful_runs"] / max(1, self.metrics["total_runs"])) * 100
        
        return {
            "overall_metrics": {
                "total_runs": self.metrics["total_runs"],
                "success_rate": f"{success_rate:.2f}%",
                "average_execution_time": f"{self.metrics['average_time']:.2f}s"
            },
            "strategy_performance": self.metrics["strategy_performance"],
            "recommendations": self._generate_recommendations()
        }
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if self.metrics["total_runs"] < 10:
            recommendations.append("è¿è¡Œæ¬¡æ•°è¾ƒå°‘ï¼Œå»ºè®®ç§¯ç´¯æ›´å¤šæ•°æ®åè¿›è¡Œä¼˜åŒ–")
        
        success_rate = (self.metrics["successful_runs"] / max(1, self.metrics["total_runs"])) * 100
        if success_rate < 90:
            recommendations.append("æˆåŠŸç‡åä½ï¼Œå»ºè®®æ£€æŸ¥é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶")
        
        if self.metrics["average_time"] > 60:
            recommendations.append("å¹³å‡æ‰§è¡Œæ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–æˆ–ä½¿ç”¨æ›´å¿«çš„ç­–ç•¥")
        
        # åˆ†ææœ€ä½³ç­–ç•¥
        best_strategy = None
        best_score = 0
        
        for strategy, metrics in self.metrics["strategy_performance"].items():
            if metrics["runs"] > 0:
                success_rate = metrics["successes"] / metrics["runs"]
                time_score = 1 / max(1, metrics["avg_time"])  # æ—¶é—´è¶ŠçŸ­åˆ†æ•°è¶Šé«˜
                overall_score = success_rate * time_score
                
                if overall_score > best_score:
                    best_score = overall_score
                    best_strategy = strategy
        
        if best_strategy:
            recommendations.append(f"æ¨èä½¿ç”¨ {best_strategy} ç­–ç•¥ä»¥è·å¾—æœ€ä½³æ€§èƒ½")
        
        return recommendations

# å…¨å±€å®ä¾‹
workflow_monitor = WorkflowMonitor()
adaptive_workflow = AdaptiveWorkflow()

if __name__ == "__main__":
    async def test_async_flows():
        """æµ‹è¯•å¼‚æ­¥å·¥ä½œæµ"""
        print("ğŸ§ª æµ‹è¯•å¼‚æ­¥æ–‡æ¡£å¤„ç†å·¥ä½œæµ")
        
        # æµ‹è¯•æ•°æ®
        test_shared = {
            "user_instruction": "è½¬æ¢ä¸ºç°ä»£å•†åŠ¡é£æ ¼çš„HTMLæ–‡æ¡£ï¼Œå›¾ç‰‡åŠ åœ†è§’è¾¹æ¡†",
            "original_document": """
# æµ‹è¯•æ–‡æ¡£

è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ï¼Œç”¨äºéªŒè¯å¼‚æ­¥å·¥ä½œæµçš„æ€§èƒ½ã€‚

## æ ¸å¿ƒç‰¹æ€§

- å¼‚æ­¥å¤„ç†
- å¹¶è¡Œæ‰§è¡Œ
- æ™ºèƒ½ä¼˜åŒ–

![æµ‹è¯•å›¾ç‰‡](test.jpg)

## æ€»ç»“

å¼‚æ­¥å·¥ä½œæµå¤§å¤§æå‡äº†å¤„ç†æ•ˆç‡ã€‚
""",
            "file_type": "markdown"
        }
        
        print("\nğŸ“Š æµ‹è¯•å®Œæ•´å¼‚æ­¥æµç¨‹...")
        complete_flow = create_async_document_flow("complete")
        
        start_time = asyncio.get_event_loop().time()
        await complete_flow.run_async(test_shared.copy())
        complete_time = asyncio.get_event_loop().time() - start_time
        
        print(f"âœ… å®Œæ•´æµç¨‹è€—æ—¶: {complete_time:.2f}ç§’")
        
        print("\nâš¡ æµ‹è¯•å¿«é€Ÿæµç¨‹...")
        quick_flow = create_async_document_flow("quick")
        
        start_time = asyncio.get_event_loop().time()
        await quick_flow.run_async(test_shared.copy())
        quick_time = asyncio.get_event_loop().time() - start_time
        
        print(f"âœ… å¿«é€Ÿæµç¨‹è€—æ—¶: {quick_time:.2f}ç§’")
        
        print(f"\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”: å¿«é€Ÿæµç¨‹æ¯”å®Œæ•´æµç¨‹å¿« {((complete_time - quick_time) / complete_time * 100):.1f}%")
        
        print("\nğŸ¤– æµ‹è¯•æ™ºèƒ½ç­–ç•¥é€‰æ‹©...")
        optimal_flow = await auto_create_optimal_flow(
            test_shared["user_instruction"],
            test_shared["original_document"]
        )
        print(f"æ™ºèƒ½é€‰æ‹©ç­–ç•¥: {optimal_flow.processing_strategy}")
        
        print("\nğŸ“Š æ€§èƒ½ç›‘æ§æŠ¥å‘Š:")
        print(workflow_monitor.get_performance_report())
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_async_flows())